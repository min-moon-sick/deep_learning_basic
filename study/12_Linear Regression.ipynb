{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acdf2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbef3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1552538c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2   3      4      5     6       7   8      9     10  \\\n",
       "0    0.00632  18.0   2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1    0.02731   0.0   7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2    0.02729   0.0   7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3    0.03237   0.0   2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4    0.06905   0.0   2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "..       ...   ...    ...  ..    ...    ...   ...     ...  ..    ...   ...   \n",
       "501  0.06263   0.0  11.93   0  0.573  6.593  69.1  2.4786   1  273.0  21.0   \n",
       "502  0.04527   0.0  11.93   0  0.573  6.120  76.7  2.2875   1  273.0  21.0   \n",
       "503  0.06076   0.0  11.93   0  0.573  6.976  91.0  2.1675   1  273.0  21.0   \n",
       "504  0.10959   0.0  11.93   0  0.573  6.794  89.3  2.3889   1  273.0  21.0   \n",
       "505  0.04741   0.0  11.93   0  0.573  6.030  80.8  2.5050   1  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "0    396.90  4.98  24.0  \n",
       "1    396.90  9.14  21.6  \n",
       "2    392.83  4.03  34.7  \n",
       "3    394.63  2.94  33.4  \n",
       "4    396.90  5.33  36.2  \n",
       "..      ...   ...   ...  \n",
       "501  391.99  9.67  22.4  \n",
       "502  396.90  9.08  20.6  \n",
       "503  396.90  5.64  23.9  \n",
       "504  393.45  6.48  22.0  \n",
       "505  396.90  7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../080228-master/080228-master/deeplearning/dataset/housing.csv', header=None, delim_whitespace= True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39c298ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df.iloc[:,:-1], df.iloc[:, -1], test_size= 0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c32db8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26829.7227 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 837.8315 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 259.3889 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 227.2657 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 203.7464 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 184.3715 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 168.2509 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 154.3981 - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 141.7624 - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 130.7445 - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 119.9960 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 110.6404 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 102.1363 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 93.6114 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 88.3663 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 84.4405 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 80.8724 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 78.3885 - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 76.2187 - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 74.1290 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 72.7687 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 71.2589 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 71.9694 - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 69.4731 - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 68.7260 - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 67.7726 - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 67.2371 - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 66.9790 - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 65.6791 - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 66.0694 - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 64.3572 - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 63.4232 - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 63.1057 - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 62.5613 - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 62.4266 - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 61.1265 - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 60.7418 - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 59.9315 - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 59.5586 - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 58.4236 - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 57.5859 - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.9312 - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 57.7447 - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.0639 - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.5626 - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 55.3678 - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 54.4772 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.4812 - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.5341 - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.0500 - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.4427 - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.7783 - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 51.7409 - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.6135 - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 57.2881 - accuracy: 0.0000e+0 - 0s 1ms/step - loss: 50.5172 - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.3232 - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.6819 - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 989us/step - loss: 48.7199 - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.9502 - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.9060 - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.8456 - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 45.5057 - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 44.2099 - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.1078 - accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 65.3095 - accuracy: 0.0000e+0 - 0s 1ms/step - loss: 42.3551 - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.0595 - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.9210 - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 40.2916 - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.0905 - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.0272 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.2305 - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.5796 - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.4569 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.0952 - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.4699 - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.5178 - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.8728 - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step - loss: 34.7335 - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.0497 - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.1707 - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.0366 - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.0202 - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.2631 - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.2994 - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.7229 - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.5680 - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.1018 - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.0327 - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.0734 - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.6639 - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.8517 - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.4255 - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.0376 - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.7398 - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.9735 - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.8592 - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.2565 - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.0695 - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.0559 - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.9203 - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.1194 - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.9305 - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.4667 - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.3755 - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.0246 - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.4375 - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.3051 - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.4329 - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.9150 - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.9372 - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3165 - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.7516 - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.5006 - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.7699 - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.4284 - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.5345 - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.1492 - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.2263 - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.2824 - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.6733 - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.8354 - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.7326 - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.9068 - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1822 - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.2915 - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1198 - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.8020 - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.6190 - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.5206 - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.6415 - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.4468 - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.6696 - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.6004 - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1801 - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.6702 - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.0620 - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.5160 - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.6190 - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.1602 - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.5769 - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.0495 - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.8957 - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.1572 - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.1052 - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.8549 - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.1336 - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.0547 - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.9500 - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.5582 - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.5002 - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.6078 - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.4158 - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.9038 - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.6104 - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step - loss: 19.9494 - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.8708 - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.3476 - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.3504 - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.5528 - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.4594 - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.1250 - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.0095 - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.3096 - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.6383 - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.1394 - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.7813 - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.8697 - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.9139 - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.9101 - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.5169 - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.5717 - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.8276 - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.6832 - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.5823 - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.1951 - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.3000 - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.6808 - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.5725 - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.1639 - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.0109 - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.9369 - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.5113 - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.2458 - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.6067 - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.6084 - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 15.8152 - accuracy: 0.0000e+0 - 0s 1ms/step - loss: 18.0943 - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.8052 - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.5336 - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.7063 - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 39.2548 - accuracy: 0.0000e+0 - 0s 1ms/step - loss: 18.0630 - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 27.4564 - accuracy: 0.0000e+0 - 0s 1ms/step - loss: 20.8898 - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 19.9290 - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.9602 - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.3344 - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.0936 - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.1367 - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.0076 - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.8839 - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.0544 - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.3497 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4256264f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim = 13, activation = 'relu'))\n",
    "model.add(Dense(6, activation= 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 200, batch_size = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "136377b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c918512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 값 : 22.6, 예측 값 : 19.910825729370117\n",
      "실제 값 : 50.0, 예측 값 : 25.380401611328125\n",
      "실제 값 : 23.0, 예측 값 : 25.306617736816406\n",
      "실제 값 : 8.3, 예측 값 : 14.502418518066406\n",
      "실제 값 : 21.2, 예측 값 : 20.169498443603516\n",
      "실제 값 : 19.9, 예측 값 : 21.703575134277344\n",
      "실제 값 : 20.6, 예측 값 : 21.129924774169922\n",
      "실제 값 : 18.7, 예측 값 : 22.731678009033203\n",
      "실제 값 : 16.1, 예측 값 : 18.27882957458496\n",
      "실제 값 : 18.6, 예측 값 : 14.251201629638672\n",
      "실제 값 : 8.8, 예측 값 : 12.236766815185547\n",
      "실제 값 : 17.2, 예측 값 : 15.999126434326172\n",
      "실제 값 : 14.9, 예측 값 : 17.354454040527344\n",
      "실제 값 : 10.5, 예측 값 : 11.346233367919922\n",
      "실제 값 : 50.0, 예측 값 : 47.19023895263672\n",
      "실제 값 : 29.0, 예측 값 : 26.743623733520508\n",
      "실제 값 : 23.0, 예측 값 : 21.725643157958984\n",
      "실제 값 : 33.3, 예측 값 : 34.992591857910156\n",
      "실제 값 : 29.4, 예측 값 : 30.30354118347168\n",
      "실제 값 : 21.0, 예측 값 : 23.713031768798828\n",
      "실제 값 : 23.8, 예측 값 : 24.131145477294922\n",
      "실제 값 : 19.1, 예측 값 : 20.144147872924805\n",
      "실제 값 : 20.4, 예측 값 : 18.520572662353516\n",
      "실제 값 : 29.1, 예측 값 : 26.118297576904297\n",
      "실제 값 : 19.3, 예측 값 : 23.334442138671875\n",
      "실제 값 : 23.1, 예측 값 : 17.18352508544922\n",
      "실제 값 : 19.6, 예측 값 : 19.392066955566406\n",
      "실제 값 : 19.4, 예측 값 : 15.966236114501953\n",
      "실제 값 : 38.7, 예측 값 : 37.277099609375\n",
      "실제 값 : 18.7, 예측 값 : 18.26767349243164\n",
      "실제 값 : 14.6, 예측 값 : 17.609622955322266\n",
      "실제 값 : 20.0, 예측 값 : 19.925745010375977\n",
      "실제 값 : 20.5, 예측 값 : 19.525482177734375\n",
      "실제 값 : 20.1, 예측 값 : 20.676687240600586\n",
      "실제 값 : 23.6, 예측 값 : 26.664827346801758\n",
      "실제 값 : 16.8, 예측 값 : 20.27031707763672\n",
      "실제 값 : 5.6, 예측 값 : 12.599723815917969\n",
      "실제 값 : 50.0, 예측 값 : 26.551979064941406\n",
      "실제 값 : 14.5, 예측 값 : 15.761028289794922\n",
      "실제 값 : 13.3, 예측 값 : 15.07791519165039\n",
      "실제 값 : 23.9, 예측 값 : 24.992368698120117\n",
      "실제 값 : 20.0, 예측 값 : 22.833662033081055\n",
      "실제 값 : 19.8, 예측 값 : 18.437902450561523\n",
      "실제 값 : 13.8, 예측 값 : 16.74523162841797\n",
      "실제 값 : 16.5, 예측 값 : 19.14431381225586\n",
      "실제 값 : 21.6, 예측 값 : 23.323617935180664\n",
      "실제 값 : 20.3, 예측 값 : 21.768394470214844\n",
      "실제 값 : 17.0, 예측 값 : 20.296613693237305\n",
      "실제 값 : 11.8, 예측 값 : 14.935680389404297\n",
      "실제 값 : 27.5, 예측 값 : 21.284753799438477\n",
      "실제 값 : 15.6, 예측 값 : 15.901134490966797\n",
      "실제 값 : 23.1, 예측 값 : 19.814319610595703\n",
      "실제 값 : 24.3, 예측 값 : 18.200241088867188\n",
      "실제 값 : 42.8, 예측 값 : 26.530052185058594\n",
      "실제 값 : 15.6, 예측 값 : 16.785314559936523\n",
      "실제 값 : 21.7, 예측 값 : 22.21879005432129\n",
      "실제 값 : 17.1, 예측 값 : 21.56785774230957\n",
      "실제 값 : 17.2, 예측 값 : 17.905046463012695\n",
      "실제 값 : 15.0, 예측 값 : 15.340396881103516\n",
      "실제 값 : 21.7, 예측 값 : 20.836795806884766\n",
      "실제 값 : 18.6, 예측 값 : 22.228050231933594\n",
      "실제 값 : 21.0, 예측 값 : 21.052806854248047\n",
      "실제 값 : 33.1, 예측 값 : 29.037864685058594\n",
      "실제 값 : 31.5, 예측 값 : 31.131135940551758\n",
      "실제 값 : 20.1, 예측 값 : 18.557132720947266\n",
      "실제 값 : 29.8, 예측 값 : 32.651187896728516\n",
      "실제 값 : 15.2, 예측 값 : 18.15857696533203\n",
      "실제 값 : 15.0, 예측 값 : 17.340566635131836\n",
      "실제 값 : 27.5, 예측 값 : 14.662738800048828\n",
      "실제 값 : 22.6, 예측 값 : 24.286144256591797\n",
      "실제 값 : 20.0, 예측 값 : 19.876256942749023\n",
      "실제 값 : 21.4, 예측 값 : 21.038311004638672\n",
      "실제 값 : 23.5, 예측 값 : 30.386974334716797\n",
      "실제 값 : 31.2, 예측 값 : 29.97831153869629\n",
      "실제 값 : 23.7, 예측 값 : 26.654376983642578\n",
      "실제 값 : 7.4, 예측 값 : 10.265117645263672\n",
      "실제 값 : 48.3, 예측 값 : 36.223670959472656\n",
      "실제 값 : 24.4, 예측 값 : 23.032541275024414\n",
      "실제 값 : 22.6, 예측 값 : 24.27899742126465\n",
      "실제 값 : 18.3, 예측 값 : 18.426185607910156\n",
      "실제 값 : 23.3, 예측 값 : 24.07281494140625\n",
      "실제 값 : 17.1, 예측 값 : 22.210731506347656\n",
      "실제 값 : 27.9, 예측 값 : 17.86093521118164\n",
      "실제 값 : 44.8, 예측 값 : 38.505340576171875\n",
      "실제 값 : 50.0, 예측 값 : 38.46676254272461\n",
      "실제 값 : 23.0, 예측 값 : 24.712207794189453\n",
      "실제 값 : 21.4, 예측 값 : 20.16303253173828\n",
      "실제 값 : 10.2, 예측 값 : 12.209785461425781\n",
      "실제 값 : 23.3, 예측 값 : 27.548940658569336\n",
      "실제 값 : 23.2, 예측 값 : 18.06671714782715\n",
      "실제 값 : 18.9, 예측 값 : 20.805749893188477\n",
      "실제 값 : 13.4, 예측 값 : 15.429832458496094\n",
      "실제 값 : 21.9, 예측 값 : 22.507034301757812\n",
      "실제 값 : 24.8, 예측 값 : 30.16388511657715\n",
      "실제 값 : 11.9, 예측 값 : 24.146257400512695\n",
      "실제 값 : 24.3, 예측 값 : 22.856027603149414\n",
      "실제 값 : 13.8, 예측 값 : 8.617301940917969\n",
      "실제 값 : 24.7, 예측 값 : 26.867576599121094\n",
      "실제 값 : 14.1, 예측 값 : 16.03619384765625\n",
      "실제 값 : 18.7, 예측 값 : 20.600744247436523\n",
      "실제 값 : 28.1, 예측 값 : 23.6929988861084\n",
      "실제 값 : 19.8, 예측 값 : 20.128643035888672\n",
      "실제 값 : 26.7, 예측 값 : 29.673721313476562\n",
      "실제 값 : 21.7, 예측 값 : 23.957361221313477\n",
      "실제 값 : 22.0, 예측 값 : 21.21919822692871\n",
      "실제 값 : 22.9, 예측 값 : 20.8893985748291\n",
      "실제 값 : 10.4, 예측 값 : 11.715812683105469\n",
      "실제 값 : 21.9, 예측 값 : 17.823524475097656\n",
      "실제 값 : 20.6, 예측 값 : 22.32767677307129\n",
      "실제 값 : 26.4, 예측 값 : 22.666606903076172\n",
      "실제 값 : 41.3, 예측 값 : 33.97093200683594\n",
      "실제 값 : 17.2, 예측 값 : 13.686962127685547\n",
      "실제 값 : 27.1, 예측 값 : 17.75299835205078\n",
      "실제 값 : 20.4, 예측 값 : 16.03719711303711\n",
      "실제 값 : 16.5, 예측 값 : 13.855846405029297\n",
      "실제 값 : 24.4, 예측 값 : 22.121675491333008\n",
      "실제 값 : 8.4, 예측 값 : 10.788738250732422\n",
      "실제 값 : 23.0, 예측 값 : 23.16050148010254\n",
      "실제 값 : 9.7, 예측 값 : 14.551177978515625\n",
      "실제 값 : 50.0, 예측 값 : 43.163475036621094\n",
      "실제 값 : 30.5, 예측 값 : 31.06789779663086\n",
      "실제 값 : 12.3, 예측 값 : 15.856925964355469\n",
      "실제 값 : 19.4, 예측 값 : 21.503978729248047\n",
      "실제 값 : 21.2, 예측 값 : 18.620418548583984\n",
      "실제 값 : 20.3, 예측 값 : 20.392948150634766\n",
      "실제 값 : 18.8, 예측 값 : 21.786144256591797\n",
      "실제 값 : 33.4, 예측 값 : 37.04073715209961\n",
      "실제 값 : 18.5, 예측 값 : 18.595972061157227\n",
      "실제 값 : 19.6, 예측 값 : 21.952728271484375\n",
      "실제 값 : 33.2, 예측 값 : 30.981822967529297\n",
      "실제 값 : 13.1, 예측 값 : 18.691740036010742\n",
      "실제 값 : 7.5, 예측 값 : 13.847480773925781\n",
      "실제 값 : 13.6, 예측 값 : 21.192943572998047\n",
      "실제 값 : 17.4, 예측 값 : 17.66580581665039\n",
      "실제 값 : 8.4, 예측 값 : 12.871784210205078\n",
      "실제 값 : 35.4, 예측 값 : 32.5092887878418\n",
      "실제 값 : 24.0, 예측 값 : 18.749523162841797\n",
      "실제 값 : 13.4, 예측 값 : 17.987377166748047\n",
      "실제 값 : 26.2, 예측 값 : 24.173137664794922\n",
      "실제 값 : 7.2, 예측 값 : 13.543998718261719\n",
      "실제 값 : 13.1, 예측 값 : 13.901165008544922\n",
      "실제 값 : 24.5, 예측 값 : 19.56826400756836\n",
      "실제 값 : 37.2, 예측 값 : 36.24971389770508\n",
      "실제 값 : 25.0, 예측 값 : 25.61336326599121\n",
      "실제 값 : 24.1, 예측 값 : 25.418025970458984\n",
      "실제 값 : 16.6, 예측 값 : 18.75115203857422\n",
      "실제 값 : 32.9, 예측 값 : 29.448001861572266\n",
      "실제 값 : 36.2, 예측 값 : 29.740110397338867\n",
      "실제 값 : 11.0, 예측 값 : 13.653133392333984\n",
      "실제 값 : 7.2, 예측 값 : 12.037216186523438\n",
      "실제 값 : 22.8, 예측 값 : 34.35803985595703\n",
      "실제 값 : 28.7, 예측 값 : 29.711484909057617\n"
     ]
    }
   ],
   "source": [
    "test_y_arr = np.array(test_y)\n",
    "\n",
    "for i in range(len(test_y)):\n",
    "    label = test_y_arr[i]\n",
    "    prediction = pred_y[i]\n",
    "    print(\"실제 값 : {}, 예측 값 : {}\".format(label, prediction) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669baeb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a452b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
