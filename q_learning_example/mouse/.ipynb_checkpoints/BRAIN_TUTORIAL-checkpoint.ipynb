{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# SARSA TABLE\n",
    "class SarsaTable():\n",
    "    def __init__(self, num_actions, learning_rate, discount_factor, epsilon_greedy):\n",
    "        self.actions = num_actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.temp_q_table = dict()\n",
    "\n",
    "    # Action 진행\n",
    "    def choose_action(self, observation):\n",
    "        observation = str(observation)\n",
    "\n",
    "        if observation in self.temp_q_table:\n",
    "            pass\n",
    "        else:\n",
    "            self.temp_q_table[observation] = [0] * self.actions\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            temp_state_action = self.temp_q_table[observation]\n",
    "            actions = temp_state_action == np.max(temp_state_action)\n",
    "            temp_action = np.random.choice([i for i in range(len(actions)) if actions[i] == True])\n",
    "        else:\n",
    "            temp_action = np.random.choice(self.actions)\n",
    "\n",
    "        self.epsilon = self.epsilon - 1e-3\n",
    "\n",
    "        return temp_action\n",
    "\n",
    "    # Learning 진행\n",
    "    def learn(self, s, a, r, s_, a_, done):\n",
    "        s = str(s)\n",
    "        s_ = str(s_)\n",
    "        temp_q_pred = self.temp_q_table[s][a]\n",
    "\n",
    "        if not done:\n",
    "            temp_q_target = r + self.gamma * self.temp_q_table[s_][a_]  # next state is not terminal\n",
    "        else:\n",
    "            temp_q_target = r\n",
    "\n",
    "        self.temp_q_table[s][a] += self.lr * (temp_q_target - temp_q_pred)  # update\n",
    "\n",
    "\n",
    "# Q TABLE\n",
    "class QTable():\n",
    "    def __init__(self, num_actions, learning_rate, discount_factor, epsilon_greedy):\n",
    "        self.actions = num_actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "        self.temp_q_table = dict()\n",
    "\n",
    "    # Action 진행\n",
    "    def choose_action(self, observation):\n",
    "        observation = str(observation)\n",
    "\n",
    "        if observation in self.temp_q_table:\n",
    "            pass\n",
    "        else:\n",
    "            self.temp_q_table[observation] = [0] * self.actions\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            temp_state_action = self.temp_q_table[observation]\n",
    "            actions = temp_state_action == np.max(temp_state_action)\n",
    "            temp_action = np.random.choice([i for i in range(len(actions)) if actions[i] == True])\n",
    "        else:\n",
    "            temp_action = np.random.choice(self.actions)\n",
    "\n",
    "        self.epsilon = self.epsilon - 1e-3\n",
    "\n",
    "        return temp_action\n",
    "\n",
    "    # Learning 진행\n",
    "    def learn(self, s, a, r, s_, a_, done):\n",
    "        s = str(s)\n",
    "        s_ = str(s_)\n",
    "        temp_q_pred = self.temp_q_table[s][a]\n",
    "\n",
    "        if not done:\n",
    "            temp_q_target = r + self.gamma * np.max(self.temp_q_table[s_])  # next state is not terminal\n",
    "        else:\n",
    "            temp_q_target = r\n",
    "\n",
    "        self.temp_q_table[s][a] += self.lr * (temp_q_target - temp_q_pred)  # update\n",
    "\n",
    "# DeepSarsa 구현\n",
    "class DeepSarsa():\n",
    "    def __init__(self, num_actions, learning_rate, discount_factor, epsilon_greedy):\n",
    "        self.actions = num_actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon_greedy\n",
    "\n",
    "        # Initialize a neural network\n",
    "        self.prepare_brain()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = np.array([observation])\n",
    "        predict_present_q = self.model.predict(observation)\n",
    "\n",
    "        print(\"현재 관측은 {} 입니다.\".format(observation))\n",
    "        print(\"관측에 따른 Q value 결과는 {} 입니다.\".format(predict_present_q))\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            temp_state_action = np.argmax(predict_present_q)\n",
    "            action = temp_state_action\n",
    "            print(\"현재 관측 결과 중, 가장 큰 값은 {} 입니다.\".format(temp_state_action))\n",
    "        else:\n",
    "            action = np.random.choice(self.actions)\n",
    "            print(\"랜덤 결과는, {} 입니다.\".format(action))\n",
    "\n",
    "        self.epsilon = self.epsilon - 1e-4\n",
    "        print(\"입실론 값\", self.epsilon)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Initialize a neural network\n",
    "    def prepare_brain(self):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(4, activation='linear')])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "        self.model.compile(optimizer=self.optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train neural network\n",
    "    def learn(self, s, a, r, s_, a_, done):\n",
    "        s = np.array([s])\n",
    "        s_ = np.array([s_])\n",
    "        present_q = self.model.predict(s)\n",
    "        predict_next_q = self.model.predict(s_)\n",
    "\n",
    "        if not done:\n",
    "            temp_q_target = r + self.gamma * predict_next_q[0][a_]\n",
    "        else:\n",
    "            temp_q_target = r\n",
    "\n",
    "        # a 빼고 전부, 그대로 사용\n",
    "        origin_label = np.array((tf.ones(4) - tf.one_hot(a, 4)) * present_q)\n",
    "        changed_label = np.array([tf.one_hot(a, 4) * temp_q_target])\n",
    "        label = origin_label + changed_label\n",
    "\n",
    "        # 한번 학습 시킴\n",
    "        self.model.fit(s, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
